network_type: feedforward

pre_torso_kwargs:
  torso_type: mlp
  layer_sizes: [128, 128]
  use_layer_norm: False
  activation: relu
